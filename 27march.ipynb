{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eec7ca0-ca59-42e5-a599-557a5cc7544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an independent variable in a regression model.\n",
    "\n",
    "#Whereas correlation explains the strength of the relationship between an independent and a dependent variable, R-squared explains the extent to which the variance of one variable explains the variance of the second variable. So, if the R2 of a model is 0.50, then approximately half of the observed variation can be explained by the model’s inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27876b12-fe32-44f7-89df-88721814e091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R-squared is a statistical measure that indicates how much of the variation of a dependent variable is explained by an independent variable in a regression model.\\nIn investing, R-squared is generally interpreted as the percentage of a fund’s or security’s price movements that can be explained by movements in a benchmark index.\\nAn R-squared of 100% means that all movements of a security (or other dependent variable) are completely explained by movements in the index (or whatever independent variable you are interested'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''R-squared is a statistical measure that indicates how much of the variation of a dependent variable is explained by an independent variable in a regression model.\n",
    "In investing, R-squared is generally interpreted as the percentage of a fund’s or security’s price movements that can be explained by movements in a benchmark index.\n",
    "An R-squared of 100% means that all movements of a security (or other dependent variable) are completely explained by movements in the index (or whatever independent variable you are interested'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd696b61-e1ce-4a34-83ad-22d238102d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate the total variance, you would subtract the average actual value from each of the actual values, square the results, and sum them. From there, divide the first sum of errors (unexplained variance) by the second sum (total variance), subtract the result from one, and you have the R-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fb48d6-2f83-48b1-bd65-6d5f4cfb1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 R-squared (R2) and adjusted R-squared are both used to evaluate the goodness of fit of a regression model. R2 represents the proportion of the variance in the dependent variable explained by the independent variables. Adjusted R-squared considers the number of predictors in the model and penalizes excessive variables, providing a more accurate measure of the model’s goodness of fit, especially with multiple predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8558abc7-53b8-4f26-b9be-7e04a769410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 When evaluating the goodness of fit of a regression model, it is better to use adjusted R-squared than R-squared.13 Adjusted R-squared is useful when there are multiple variables in the regression model, allowing us to compare models with differing numbers of independent variables.1 It is also helpful when not comparing models to compare the unadjusted R-squared with the adjusted R-squared to see how the R-squared (raw) is inflated due to extra/superfluous terms in the model.0 R-squared measures the variation of a regression model, while adjusted R-squared measures the variation for a multiple regression model and helps determine goodness of fit.13 Adjusted R-squared can be used to tell if adding predictors to a model is adding useless complexity.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76bdbb1e-e3c0-42fa-9f71-b7f99bcb522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "370b8149-ca1d-4f7a-8fa4-cf3643adab0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mean Squared Error(MSE) and Root Mean Square Error penalizes the large prediction errors vi-a-vis Mean Absolute Error (MAE). However, RMSE is widely used than MSE to evaluate the performance of the regression model with other random models as it has the same units as the dependent variable (Y-axis).\\nMSE is a differentiable function that makes it easy to perform mathematical operations in comparison to a non-differentiable function like MAE. Therefore, in many models, RMSE is used as a default metric for calculating Loss Function despite being harder to interpret than MAE.\\nThe lower value of MAE, MSE, and RMSE implies higher accuracy of a regression model. However, a higher value of R square is considered desirable.\\nR Squared & Adjusted R Squared are used for explaining how well the independent variables in the linear regression model explains the variability in the dependent variable. R Squared value always increases with the addition of the independent variables which might lead to the addition of the redundant variables in our model. However, the adjusted R-squared solves this problem.\\nAdjusted R squared takes into account the number of predictor variables, and it is used to determine the number of independent variables in our model. The value of Adjusted R squared decreases if the increase in the R square by the additional variable isn’t significant enough.\\nFor comparing the accuracy among different linear regression models, RMSE is a better choice than R Squared.\\nConclusion\\n\\nBoth RMSE and R- Squared quantifies how well a linear regression model fits a dataset. The RMSE tells how well a regression model can predict the value of a response variable in absolute terms while R- Squared tells how well the predictor variables can explain the variation in the response variable.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.\n",
    "#Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.\n",
    "#Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.\n",
    "'''Mean Squared Error(MSE) and Root Mean Square Error penalizes the large prediction errors vi-a-vis Mean Absolute Error (MAE). However, RMSE is widely used than MSE to evaluate the performance of the regression model with other random models as it has the same units as the dependent variable (Y-axis).\n",
    "MSE is a differentiable function that makes it easy to perform mathematical operations in comparison to a non-differentiable function like MAE. Therefore, in many models, RMSE is used as a default metric for calculating Loss Function despite being harder to interpret than MAE.\n",
    "The lower value of MAE, MSE, and RMSE implies higher accuracy of a regression model. However, a higher value of R square is considered desirable.\n",
    "R Squared & Adjusted R Squared are used for explaining how well the independent variables in the linear regression model explains the variability in the dependent variable. R Squared value always increases with the addition of the independent variables which might lead to the addition of the redundant variables in our model. However, the adjusted R-squared solves this problem.\n",
    "Adjusted R squared takes into account the number of predictor variables, and it is used to determine the number of independent variables in our model. The value of Adjusted R squared decreases if the increase in the R square by the additional variable isn’t significant enough.\n",
    "For comparing the accuracy among different linear regression models, RMSE is a better choice than R Squared.\n",
    "Conclusion\n",
    "\n",
    "Both RMSE and R- Squared quantifies how well a linear regression model fits a dataset. The RMSE tells how well a regression model can predict the value of a response variable in absolute terms while R- Squared tells how well the predictor variables can explain the variation in the response variable.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa5be29d-2945-4398-97b7-0efa080edc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 RMSE is usually the preferred metric over MAE for measuring model performance. This is because developers often want to reduce the occurrence of large outliers in their predictions and MAE can be seen as too simplistic for understanding overall model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0b9c15-783b-448a-8a4a-69331143cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean square error advantages\n",
    "#equation is differntiable ,it has only one local or global minima\n",
    "#disadvantages\n",
    "#not robust to outlier\n",
    "#it us not in same unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "632584f0-5764-45aa-a324-a33c19af4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean absolute error\n",
    "#advantages\n",
    "#robust to outlier\n",
    "#it will be in the same unit\n",
    "#disadvantages\n",
    "#converge usaually take more time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a362f5-ca6b-4166-a650-bb22695e0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "#advantages\n",
    "#same unit \n",
    "#differntiable\n",
    "#disadvantages\n",
    "#not robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f086aa8-1ecb-4d13-94d0-fa6a23f8de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 LASSO stands for Least Absolute Shrinkable and Selection Operator. As mentioned in the regularization definition, it is the process of adding information to prevent the over-fitting problem, so a small modification will be done to the cost function of the ordinary least square\n",
    "# Minimizing the slope means that it makes the line less steep and this will not make the line passing through all data points and this will help to prevent over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b0f630c-9541-4c7e-8ac5-a3b953a24694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regularization is another variation for LASSO as the term added to the cost function\n",
    "#In Ridge regularization, the penalty term can approach zero but will not be zero as it squares the coefficient (slope). Ridge Regularization cannot be used in feature selection, so when to use LASSO and Ridge?\n",
    "#If you have many features with high correlation and you need to take away the useless features then LASSO is the better solution.\n",
    "#If the number of features greater than the number of observations and many features with multi-collinearity, Ridge regularization is a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e810036a-a3a4-4d77-a1f3-48c9766ed4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfitting impacts the accuracy of Machine Learning models. The model attempts to capture the data points that do not represent the accurate properties of data. These data points may be considered as noise. To avoid the occurrence of overfitting, we may use a method called regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b17a929-1673-40c9-8907-e129295e8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indeed, even the best information doesn't recount a total story. Regression investigation is ordinarily utilized in examination to set up that a relationship exists between variables. However, correlation isn't equivalent to causation: a connection between two variables doesn't mean one causes the other to occur. Indeed, even a line in a simple linear regression that fits the information focuses well may not ensure a circumstances and logical results relationship. \n",
    "#Utilizing a linear regression model will permit you to find whether a connection between variables exists by any means. To see precisely what that relationship is and whether one variable causes another, you will require extra examination and statistical analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f76dc91-009e-4b9e-ae9d-53945590c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 model B is choosen becuase mae is low in in model so it is helpful for the model accucarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97619211-35fc-4279-8926-95ad5c5559ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10#If you have many features with high correlation and you need to take away the useless features then LASSO is the better solution.\n",
    "#If the number of features greater than the number of observations and many features with multi-collinearity, Ridge regularization is a better solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36554798-d28c-4326-bb52-01d78eb54e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 model A has ridge regulaziation so it has ha many features with multi colineairty so "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
